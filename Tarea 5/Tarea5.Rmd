---
title: "Tarea 5"
output:
  html_document:
    df_print: paged
---

![](banner.png)

<center> <h1>Tarea 5: Bayesian Inference Part II</h1> </center>
<center><strong>CC6104: Statistical Thinking</strong></center>
#### **Integrantes :** 

- Tomas Alvarado
- Sebastian Brzovic

#### **Cuerpo Docente:**

- Profesor: Felipe Bravo M.
- Auxiliar: Ignacio Meza D.
            

#### **Fecha límite de entrega:**

### **Índice:**

1. [Objetivo](#id1)
2. [Instrucciones](#id2)
3. [Referencias](#id3)
2. [Primera Parte: Preguntas Teóricas](#id4)
3. [Segunda Parte: Elaboración de Código](#id5)

### **Objetivo**<a name="id1"></a>

Bienvenid@s a la uuuuultima tarea del curso Statistical Thinking. Esta tarea tiene como objetivo evaluar los contenidos teóricos de la ultima parte del curso, los cuales se enfocan principalmente en aplicar inferencia bayesiana para generar regresiones lineales y estudiar métodos de obtención de la posterior mas poderosos, como es MCMC. Si aún no han visto las clases, se recomienda visitar los enlaces de las referencias.

La tarea consta de una parte teórica que busca evaluar conceptos vistos en clases. Seguido por una parte práctica con el fin de introducirlos a la programación en R enfocada en el análisis estadístico de datos. 

### **Instrucciones:**<a name="id2"></a>

- La tarea se realiza en grupos de **máximo 2 personas**. Pero no existe problema si usted desea hacerla de forma individual.
- La entrega es a través de u-cursos a más tardar el día estipulado en la misma plataforma. A las tareas atrasadas se les descontará un punto por día.
- El formato de entrega es este mismo **Rmarkdown** y un **html** con la tarea desarrollada. Por favor compruebe que todas las celdas han sido ejecutadas en el archivo html.
- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación.
- No serán revisadas tareas desarrolladas en Python.
- Está **PROHIBIDO** la copia o compartir las respuestas entre integrantes de diferentes grupos.
- Pueden realizar consultas de la tarea a través de U-cursos y/o del canal de Discord del curso. 


### **Referencias:**<a name="id3"></a>

Slides de las clases:

- [Bayesian Linear Regression](https://github.com/dccuchile/CC6104/blob/master/slides/3_3_ST-bayes_lin.pdf)
- [Markov Chain Monte Carlo](https://github.com/dccuchile/CC6104/blob/master/slides/3_4_ST-MCMC.pdf)
- [Model Evaluation and Information Criteria](https://github.com/dccuchile/CC6104/blob/master/slides/4_1_ST-eval.pdf)
- [Directed Graphical Models](https://github.com/dccuchile/CC6104/blob/master/slides/4_2_ST-dag.pdf)


Videos de las clases:

- Bayesian Linear Regression: [video 1](https://youtu.be/DrwhRshBVjM), [video 2](https://youtu.be/lgNMDCzTV9k),  [video 3](https://youtu.be/ajMucPrZDpU), [video 4](https://youtu.be/YSGWWSUMPOk), [video 5](https://youtu.be/Ma9V8Nown9Q)
- Markov Chain Monte Carlo: [video 1](https://youtu.be/gsofPiPBIeU), [video 2](https://youtu.be/EJZWaph61p4),  [video 3](https://youtu.be/jfidS22imJM), [video 4](https://youtu.be/kif9EG-sy2I), [video 5](https://youtu.be/iVgiowZvyZM), [video 6](https://youtu.be/r0BNqctisLg)
- Model Evaluation and Information Criteria: [video 1](https://youtu.be/HCCzwltLVCc), [video 2](https://youtu.be/twpZHZMmKgM),  [video 3](https://youtu.be/ny4SlO3rcTo), [video 4](https://youtu.be/6U7laePWt9M), [video 5](https://youtu.be/vE2VaK9tLV8), [video 6](https://youtu.be/wmBugs36H-4)  
- Directed Graphical Models: [video 1](https://youtu.be/2jnj-7xpK0E), [video 2](https://youtu.be/GZf8uB37noU),  [video 3](https://youtu.be/3EDdNLOrj_4), [video 4](https://youtu.be/cODS9GgepA4), [video 5](https://youtu.be/JA8H-LjAatE), [video 6](https://youtu.be/YXf0wnzvCFM)   

Documentación:

- [rethinking](https://github.com/rmcelreath/rethinking)
- [tidyr](https://tidyr.tidyverse.org)
- [purrr](https://purrr.tidyverse.org)
- [dplyr](https://dplyr.tidyverse.org)
- [ggplot2](https://ggplot2.tidyverse.org/)

# Primera Parte: Preguntas Teóricas<a name="id4"></a>
A continuación, se presentaran diferentes preguntas que abordan las temáticas vistas en clases. Por favor responda cada una de estas preguntas de forma breve, no más de 4 o 5 lineas.

#### **Pregunta 1:**

Señale algunos beneficios (no mas de dos) que nos brinda realizar una regresión lineal bayesiana sobre una regresión ajustada por mínimos cuadrados.

> Regresion Bayesiana es mas flexible que una regresion ajustada por minimos cuadrados debido a que incorpora informacion a priori. hace que los modelos sean mas robusto y menos proclive a que hagan overfitting. 
Otro beneficio es que permite interpretar la incertidumbre de una manera mas "facil", debido a que tenemos mas elementos, no solo intervalo de confianza sino tambien, tenemos el postirior, posterior etc.


#### **Pregunta 2:**

A continuación se presenta un modelo de regresión lineal bayesiana:

$$y_i \sim Normal(\mu, \sigma)$$
$$\mu = \beta_0 + \beta_1*x$$
$$\beta_0 \sim Normal(10,2)$$
$$\beta_1 \sim Normal(0,1)$$
$$\sigma \sim Uniform(0,50)$$

En base al modelo presentado, responda las siguientes preguntas:

- [ ] Describa que representa cada una de las lineas del modelo.

- [ ] Señale los parámetros que conforman a la regresión lineal. 

- [ ] Que objetivo cumple en el modelo lineal tener una distribución $Normal(0,1)$ en el parámetro $\beta_1$.

- [ ] Que propiedad de las regresiones lineales nos entrega $sigma$.

**Respuesta:**

> El primero corresponde a la likelihood en donde cada punto (u= b0 +b1*x1 + ...) sera modelada por la gausiana con media u y desviation o.
El segundo corresponde al modelo lineal para cada u_i con un parametros b1, b0. 
La tercera línea corresponde al Prior del parámetro b0,  con media 10 y desviacion 2.
La cuarta linea corresponde al Prior del parámetro b1,  con media 0 y desviacion 1 
Y finalmente, sigma es la deviacion estandar de mis normales de cada media es una uniforme entre 0 y 50

#### **Pregunta 3:**

Explique de forma resumida como funciona el algoritmo de Laplace approximation utilizado para la regresión lineal. Señale el porque existe la necesidad de aplicar este modelo y los supuestos que se realizan con este método.

**Respuesta:**

> Laplace Aproximation es asumir que mi joint posterior es una gausiana multivariada, donde mu son los valores que maximiza la posterior. 
Se usa porque para cualquier modelo bastante complejo  una descripción exacta del posterior es computacionalmente intratable. Más allá de una evaluación exhaustiva, la inferencia aproximada permite recuperar descripciones razonables de una superficie posterior o de coste a partir de métodos de aproximación. Por lo que la aproximación de Laplace proporciona el método determinista más simple disponible.

#### **Pregunta 4:**
Determine si las siguientes afirmaciones son verdaderas o falsas. Justifique las falsas.

- [V] El algoritmo de metropolis hasting solo funciona si la probabilidad de saltar de B a A es la misma que de A a B.
- [F] El algoritmo de Gibbs funciona en cualquier caso.
- [V] El algoritmo de metropolis hasting y de Gibbs son el mismo algoritmo, pero este ultimo es una variante del primero.

> Verdadero, el algoritmo converge por la condicion de que  q(o(a)|o(b)) = q(o(a)|o(a))
Falso, en teoria funcionaria, pero es muy ineficiente con muchisimo datos (ya mas de miles de datos deja de ser eficiente)
Verdadero

#### **Pregunta 5:**

El algoritmo de Gibbs es mas eficiente que el de metropolis hasting. ¿Como se logra este efecto? ¿Existe alguna limitante de esta estrategia?

> Es mas efectivo porque hace proposiciones adaptativas, es decir la distribucion de los candidatos no es fija, va iterando. Se ajusta inteligentemente el proces de muestreo dependiendo de los parametros que se va actualizando la posterior en cada momento. Ademas que en Metropolisis algorithm muchas proposiciones son rechazadas.
Sus limitantes son que aveces no queremos usar prior conjugados, ademas se pone muy ineficiente cuando tenemos miles de datos.


#### **Pregunta 6:**

De una ventaja y una desventaja del algoritmo HMC.

> HMC es mas eficiente puesto es capaz de describir la posterior con mucho menos datos, por lo que es mejor con demasiados datos
HMC es bastante complejo y caro

#### **Pregunta 7:**

Nombre y explique dos propiedades que son deseables en una cadena de Markov.

> Estacionaridad: que vaya convergiendo a su distribucion estacionaria, a la posterior. Que se quede ahi. 
Buena mezcla: Que exista poca autocorrelacion, que no se noten mucho patrones, que cada ejemplo dependa del anterior.

#### **Pregunta 8:**
Explique cómo cross-validation, criterios de información y regularización ayudan a mitigar o medir los problemas de underfitting y overfitting.

> Cross-validation utiliza trozos de datos (folds) para poder obtener un estimado del out-of-sample deviance. Criterios de información utiliza la siguiente formula AIC=Dtrain+2p. Regularization utiliza el parametro delta para corregir el overfitting y restringe el parametro beta, realiza una restriccion al parametro β para el underfitting

#### **Pregunta 9:**

Diseñe una DAG para un problema causal inventado por usted de al menos 5 nodos (puede basarse en el ejemplo de Eugene Charniak) usando **Dagitty**  y considere que la DAG tenga al menos: una chain, un fork y un collider. Muestre con dagitty todas las independencias condicionales de su DAG. Explique las independencias usando las reglas de d-separación.

> Respuesta Aquí

---

# Segunda Parte: Elaboración de Código<a name="id5"></a>
En la siguiente sección deberá resolver cada uno de los experimentos computacionales a través de la programación en R. Para esto se le aconseja que cree funciones en R, ya que le facilitará la ejecución de gran parte de lo solicitado.

Para el desarrollo preste mucha atención en los enunciados, ya que se le solicitará la implementación de métodos sin uso de funciones predefinidas. Por otro lado, Las librerías permitidas para desarrollar de la tarea 4 son las siguientes:

```{r, eval=FALSE}
# Manipulación de estructuras
library(tidyverse)
library(dplyr)
library(tidyr)

# Para realizar plots
library(scatterplot3d)
library(ggplot2)
library(plotly)

# Manipulación de varios plots en una imagen.
library(gridExtra)

# Análisis bayesiano
library("rethinking")
```

Si no tiene instalada la librería “rethinking”, ejecute las siguientes líneas de código para instalar la librería:

```{r, eval=FALSE}
install.packages("rethinking")
```

En caso de tener problemas al momento de instalar la librería con el código anterior, utilice las siguiente chunk:

```{r, eval=FALSE}
install.packages(c("mvtnorm","loo","coda"), repos="https://cloud.r-project.org/",dependencies=TRUE)
options(repos=c(getOption('repos'), rethinking='http://xcelab.net/R'))
install.packages('rethinking',type='source')
```


### Pregunta 1: Regresión Lineal Bayesiana

El objetivo de esta pregunta es introducirlo en la aplicación de una regresión bayesiana. Con esto, buscaremos entender como calcular una regresión bayesiana en base al "motor" aproximación de Laplace, revisando como se comporta la credibilidad de sus predicciones y como la regresión lineal puede llegar a mutar a aplicar una transformación en el vector $x$. Para responder esta pregunta centre su desarrollo solo en las clases y las funciones que nos brinda la librería `rethinking`.

Unos expertos en alometría deciden realizar un estudio de las alturas de unos niños en un colegio, buscando generar un regresor lineal bayesiano capaz de predecir la altura en base al peso de los alumnos. Para realizar este trabajo recopilan los datos `table_height.csv`, quien posee observaciones fisiológicas de 192 alumnos.

**Parte I**

En conocimiento los datos recopilados por los expertos, le solicitan realizar la siguiente serie de tareas:

- [ ] Defina un modelo de regresión bayesiana, definiendo sus propios priors. Comente cada una de sus asunciones y señale a través de ecuaciones el modelo. Para definir los priors puede ser interesante la información recopilada en el siguiente link: [Priors](https://stacks.cdc.gov/view/cdc/100478)
- [ ] Ajuste el modelo lineal utilizando el método de Laplace approximation. Estudie a través del comando `precis` los resultados obtenidos y coméntelos.
- [ ] Gráfique el MAP de regresión lineal obtenida, añadiendo al gráfico el intervalo del $95\%$ que se tiene sobre la media y los valores predecidos de la altura. Comente los resultados obtenidos y señale si su modelo logra ser un buen predictor de los valores estudiados.

```{r}
library(rethinking)
data_alturas <- read.csv("table_height.csv")
head(data_alturas)

#histograma de alturas y edad
hist(data_alturas$age)

# Lo siguiente que vamos a hacer es filtrar a los niños menores de 5 años, ya que estos no estan en el colegio
data <- data_alturas[data_alturas$age >= 5,]

# De acuerdo a lo anterior hacemos un histograma con el peso
hist(data$weight)

# Sacamos el promedio y desviacion estandar del peso
print(paste("weight mean: ", mean(data$weight)))
print(paste("weight sd: ", sd(data$weight)))

# Hacemos un histograma con los datos actuales
hist(data$height)

# Sacamos el promedio y la desviacion estandar de la altura
print(paste("height mean: ",mean(data$height)))
print(paste("height sd: ",sd(data$height)))

```

```{r}
data.reg <- quap(
  # likelihood
  alist(height ~ dnorm(mu, sigma),
  # modelo de regresion 
  mu <- b0 + b1*weight,
  # prior de B0
  b0 ~ dnorm(122.5, 18),
  # prior de B1 normalizado
  b1 ~ dnorm(0, 1),
  # prior de sigma, limita los valores a positivos con maximo la ds del prior de B0
  sigma ~ dunif(0, 20)), data=data
)

precis(data.reg, prob = 0.95)

```

```{r}
post <- extract.samples(data.reg, n= 1e4)

library(MASS)
```

```{r}

# secuencia de pesos
weight.seq <- seq(from=10 , to=50 , by=1 )

mu.link <- function(weight) post$b0 + post$b1*weight
mu <- sapply( weight.seq , mu.link )

mu.mean <- apply( mu , 2 , mean )
mu.HPDI <- apply( mu , 2 , HPDI , prob=0.95 )

plot(height ~ weight , data=data , col=col.alpha(rangi2, 0.5))

lines(weight.seq, mu.mean)
# intentamos hacerle plot al shade con 95% HPPDI
#shade(mu.HPDI, weight.seq)
#por alguna razon la funcion shade no me funciona :c
```

```{r}


mu <- link(data.reg ,data=data.frame(weight=weight.seq), n=1e4 )



# ahora usando la distribucion posterior
height.weight <- function(weight) 
  rnorm(
    n=nrow(post) ,
    mean=post$b0 + post$b1*weight ,
    sd=post$sigma )

simulation.height <- sapply( weight.seq , height.weight)


height.HPDI <- apply(simulation.height, 2, HPDI, prob=0.95)


plot(height ~ weight, data, col=col.alpha(rangi2,0.5))

lines(weight.seq , mu.mean )

#shade(mu.HPDI, weight.seq )
#por alguna razon la funcion shade no me funciona :c
# draw HPDI region for simulated heights
#shade(height.HPDI, weight.seq )


#FIN parte I


```

Una vez terminado esto, podemos decir que por un lado el modelo logra acercarse a la forma de los datos, pero no logra ajustarse de manera correcta, esto dado que estos datos no tienen forma lineal definida, por lo que si se tiene un modelo lineal no logra representar los datos de los estudiantes.


**Parte II**

En base a los resultados obtenidos, el experto que trabaja con usted le señala que las alturas se suelen modelas con pesos logarítmicos, por lo que le sugiere añadir un logaritmo natural en el vector $x$ que compone su modelo lineal. Realice nuevamente la regresión utilizando un intervalo del $95\%$ sobre la media y los valores predecidos de la altura. Comente los resultados obtenidos, señalando si el modelo logra ajustar mejor los valores.

**Respuesta:**

```{r}
# Volveremos a definirnos las variables y les aplicaremos Ln

data$weight <- log(data$weight)

hist(data$weight)

print(paste("weight mean: ", mean(data$weight)))
print(paste("weight sd: ", sd(data$weight)))
hist(data$height)

print(paste("height mean: ",mean(data$height)))
print(paste("height sd: ",sd(data$height)))
#Aquí podemos ver que el modelo ahora varía la pendiente de la regresion al reducirse la distancia entre los pesos. Entonces b0 y sigma no cambian, pero b1 aumenta. 

data.reg <- quap(
  alist(height ~ dnorm(mu, sigma),
  mu <- b0 + b1*weight,
  b0 ~ dnorm(122.5, 18),
  b1 ~ dnorm(30, 10),   
  sigma ~ dunif(0, 20)), data=data
)

post <- extract.samples(data.reg, n= 1e4)

weight.seq <- seq(from=log(10) , to=log(50) , by= 0.4 )



mu.link <- function(weight) post$b0 + post$b1*weight
mu <- sapply( weight.seq , mu.link )

mu.mean <- apply( mu , 2 , mean )
mu.HPDI <- apply( mu , 2 , HPDI , prob=0.95 )

plot(height ~ weight , data=data , col=col.alpha(rangi2, 0.5))

lines(weight.seq, mu.mean)

#shade(mu.HPDI, weight.seq) 
# shade no funciona




mu <- link(data.reg ,data=data.frame(weight=weight.seq), n=1e4 )



height.weight <- function(weight) 
  rnorm(
    n=nrow(post) ,
    mean=post$b0 + post$b1*weight ,
    sd=post$sigma )

sim.height <- sapply( weight.seq , height.weight)



height.HPDI <- apply(sim.height, 2, HPDI, prob=0.95)


plot(height ~ weight, data, col=col.alpha(rangi2,0.5))

lines(weight.seq , mu.mean )

#shade(mu.HPDI ,weight.seq )

#shade(height.HPDI ,weight.seq )
# no funcionan

```
Comparando este modelo con el anterior podemos notar que este representa de manera acertada tanto los datos de los pesos como de las alturas de los estudiantes. Concluyendo asi que los pesos de los estudiantes tienen una relacion logaritmica con sus alturas. 

#### **Pregunta 2:** MCMC

El objetivo de esta pregunta es lograr samplear, mediante la técnica de MCMC, la distribución gamma. 

En general la distribución gamma se denota por $\Gamma(\alpha,\beta)$ donde $\alpha$ y $\beta$ son parámetros positivos, a $\alpha$ se le suele llamar "shape" y a $\beta$ rate La densidad no normalizada de una distribución gamma esta dada por:

$$
f(x\mid \alpha,\beta) = 
\begin{cases}
 x^{\alpha -1}e^{-\beta x} ~ &\text{ si } x > 0\\
0 ~&\text{si } x \leq 0
\end{cases}
$$

- [ ] Implemente el algoritmo de metropolis hasting utilizando $q(\theta^{(t)} \mid \theta^{(t-1)}) = \mathcal{N}(\theta^{t-1},1)$, **importante** su función tiene que poder recibir:
  - [ ] La condición inicial $\theta_{0}$.
  - [ ] Cantidad de repeticiones.
  - [ ] $\alpha$ y $\beta$.
  
  Escriba el algoritmo sin utilizar implementenaciones de la distribución gamma en r. 
  
De ahora en adelante considere $\alpha = 5$ y $\beta = \frac{1}{5}$.

  - [ ] Considere $\theta_{0} = 1$, grafique el histograma que obtiene para distintas cantidad de repeticiones, entre sus pruebas tiene que tener al menos una que tenga del orden de $10^5$ repeticiones ¿Como cambia la distribución en funcion de las repeticiones?
  - [ ] Considere distintos valores de $\theta_{0}$ y una cantidad de repeticiones grande (del orden de $10^5$), grafique las distribuciones que obtenga comente sus resultados  ¿es importante la condición inicial del algoritmo?.
  - [ ] Utilizando $\theta_{0}$ y cantidad de repeticiones conveniente (de acuerdo a lo que obtuvo en las partes anteriores) compare con la distribución real. **Obs:** En esta parte puede utilizar la distribución gamma que tiene implementado r para comparar con lo que usted realizo.

**Respuesta:**

> Respuesta Aquí

```{r}
gamma_no_normalizada <- function(alpha, beta, x){
  if(x > 0){
    return ((x ^ (alpha - 1)) * (exp(-beta * x)))
  } else{
    return(0)
  }
}

met <-function(theta0, rep, alpha, beta) {
  gamma <- c(theta0)
  for ( i in 2:rep ) {
    current <- gamma[i-1]
    proposal <- rnorm(1, current, 1)
    r <- (gamma_no_normalizada(alpha, beta, proposal)) / (gamma_no_normalizada(alpha, beta, current))
    prob_move <- min(r, 1)
    decision <- rbinom(1, 1, prob_move)
    gamma <- c(gamma, ifelse(decision == 1 , proposal , current))
  }

  return(gamma[2:rep])
  
} 

```

Teniendo lo anterior ya listo, osea nuestra funcion gamma con el MCMC, veremos la distribucion en diferentes casos y junto a esto compararemos su comportamiento, con 100, 1000, 10000 y 100000 repeticiones.

```{r}
alpha <- 5
beta <- 1/5
theta_0 <- 1
rep <- 100
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")
```



```{r}
#Con 1000 repeticiones 
rep <- 1000
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")

#Con 10000 repeticiones 
rep <- 10000
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")

#Con 100000 repeticiones 
rep <- 100000
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")
```
Ahora, dado lo anterior podemos notar que a medida que se incrementa el numero de repeticiones, la distribucion se vuelve cada vez mas clara y marcada.

Luego, veremos como se comporta esta para distintos valores de Theta 0, pero con la cantidad de repeticiones fija en 100000.

```{r}
rep <- 100000
#theta = 10
theta_0 <- 10
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")

#theta = 50
theta_0 <- 50
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")

#theta = 100
theta_0 <- 100
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")

#theta = 1000
theta_0 <- 1000
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")

```
**cambiar**La condición inicial  es importante porque a medida que aumenta theta0, aumenta el número de valores extremos o raros. Esto se debe a que el punto de partida de la función gamma parte de un valor extremo y  va a los valores de densidad más altos.

Por último, vamos a comparar con la funcion Gamma nativa de R

```{r}
theta_0 <- 1
positions <- met(theta_0, rep, alpha, beta)
simplehist(positions, xlab="x", ylab="gamma(x)")

a <- rgamma(rep, rate = beta, shape = alpha)
simplehist(a, xlab="x", ylab="gamma(x)")

```
Finalmente, podemos concluir que ambas distribuciones son equivalentes y representan la misma función de densidad.



&nbsp;
<hr />
<p style="text-align: center;">A work by <a href="https://github.com/dccuchile/CC6104">CC6104</a></p>

<!-- Add icon library -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">

<!-- Add font awesome icons -->
<p style="text-align: center;">
    <a href="https://github.com/dccuchile/CC6104"><i class="fab fa-github" style='font-size:30px'></i></a>
</p>

<p style="text-align: center;">
    <a href="https://discord.gg/XCbQvGs3Uf"><i class="fab fa-discord" style='font-size:30px'></i></a>
</p>

&nbsp;